{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CRNNs for footstep classification"
      ],
      "metadata": {
        "id": "b1xyztw5AIiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset (esc50) which contains footstep audioclips\n",
        "\n",
        "ONLY NEEDS TO BE RUN ONCE"
      ],
      "metadata": {
        "id": "b7OT8XZJAUP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/karoldvl/ESC-50/archive/master.zip -O esc50.zip\n",
        "!unzip esc50.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WDLCNkyE_oAM",
        "outputId": "745bb7a4-23fd-4592-c69d-413bac3bc97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-30 13:12:43--  https://github.com/karoldvl/ESC-50/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/karolpiczak/ESC-50/archive/master.zip [following]\n",
            "--2025-11-30 13:12:43--  https://github.com/karolpiczak/ESC-50/archive/master.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/karolpiczak/ESC-50/zip/refs/heads/master [following]\n",
            "--2025-11-30 13:12:44--  https://codeload.github.com/karolpiczak/ESC-50/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.116.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.116.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘esc50.zip’\n",
            "\n",
            "esc50.zip               [             <=>    ] 615.78M  14.7MB/s    in 39s     \n",
            "\n",
            "2025-11-30 13:13:23 (15.6 MB/s) - ‘esc50.zip’ saved [645695005]\n",
            "\n",
            "Archive:  esc50.zip\n",
            "33c8ce9eb2cf0b1c2f8bcf322eb349b6be34dbb6\n",
            "replace ESC-50-master/.circleci/config.yml? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "xoKdP2gUAel0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data handling\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# specifically for handling audio\n",
        "import librosa\n",
        "\n",
        "# machine learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "\n",
        "# for saving the model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "f7xJmAMJ_rJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply settings"
      ],
      "metadata": {
        "id": "4LMsSlKcFvDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_DIR = \"ESC-50-master/audio\"\n",
        "SR = 22050\n",
        "BATCH_SIZE = 8\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 512\n",
        "EPOCHS = 3"
      ],
      "metadata": {
        "id": "-VmX3u1OFuCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and format the dataset\n",
        "\n",
        "The esc-50 dataset contains other classes besides \"footsteps\". This code assigns \"1\" to the \"footsteps\" class, and \"0\" to every other class.\n",
        "\n",
        "Here, we also use a dataset object. While the ESC-50 dataset is still fairly small, larger datasets quickly encounter problems when being loaded into RAM all at once. The dataset object handles this by loading the data in stages, only loading the files which the model is currently using."
      ],
      "metadata": {
        "id": "k5pjKsUpAh0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download dataset\n",
        "metadata = pd.read_csv(\"ESC-50-master/meta/esc50.csv\")\n",
        "\n",
        "#assign labels\n",
        "metadata[\"binary_label\"] = (metadata[\"category\"] == \"footsteps\").astype(int)\n",
        "\n",
        "# loads audio files from dataset\n",
        "def load_audio(filename):\n",
        "    path = os.path.join(AUDIO_DIR, filename)              # finds file\n",
        "    audio, sr = librosa.load(path, sr=SR)                 # sr=SR automatically resamples\n",
        "    audio = tf.convert_to_tensor(audio, dtype=tf.float32) # prepares data to be fed into model\n",
        "    return audio\n",
        "\n",
        "# generates dataset\n",
        "def make_dataset(df):\n",
        "  def gen():\n",
        "      for _, row in df.iterrows():        # iterate over each row of the metadata\n",
        "          audio = load_audio(row[\"filename\"])   # load each file referenced in the metadata\n",
        "          label = row[\"binary_label\"]           # store the binary label\n",
        "          yield audio, label                    # return a single file after each iteration\n",
        "\n",
        "  dataset = tf.data.Dataset.from_generator(\n",
        "      gen,                                                  # our generator function, returning 1D waveform and label pairs\n",
        "      output_signature=(                                    # tells TensorFlow the shapes and datatypes of what our function yields\n",
        "          tf.TensorSpec(shape=(None,), dtype=tf.float32),   # variable-length waveform\n",
        "          tf.TensorSpec(shape=(), dtype=tf.int32)           # Scalar label (0 or 1)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # pad the dataset so that each audio file is the same length (within the batch)\n",
        "  dataset = dataset.padded_batch(\n",
        "      batch_size=BATCH_SIZE,\n",
        "      padded_shapes=([None], [])  # only pad the audio data ([None]), the labels don't need padding ([])\n",
        "    )\n",
        "  return dataset\n",
        "\n",
        "# split the data into train / validation\n",
        "train_df, test_df = train_test_split(\n",
        "    metadata,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=metadata['binary_label']  # ensures balance between classes\n",
        ")\n",
        "\n",
        "train_dataset = make_dataset(train_df)\n",
        "test_dataset = make_dataset(test_df)\n",
        "\n",
        "# Optional: shuffle & prefetch for better performance\n",
        "train_dataset = train_dataset.shuffle(200).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n"
      ],
      "metadata": {
        "id": "att4ApSoAGWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectrograms\n",
        "Keras/Tensorflow contains functionality to be able to write custom layers. As there is no predefined \"spectrogram\" layer, we must write our own. This code applies creates a custom spectrogram layer for use later in the Keras Sequential model.\n",
        "\n",
        "It should be noted that we use a log spectrogram here since human hearing is logarithmic. This generally performs better on audio within the range of human hearing."
      ],
      "metadata": {
        "id": "i6VhAJgYBtXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Spectrogram(layers.Layer):\n",
        "    def __init__(self, n_fft=1024, hop_length=512):\n",
        "        super().__init__()\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def call(self, audio):\n",
        "        stft = tf.signal.stft(\n",
        "            audio,                            # data to pre-process\n",
        "            frame_length=self.n_fft,          # length of section to apply FFT to\n",
        "            frame_step=self.hop_length,       # stride length\n",
        "            window_fn=tf.signal.hann_window   # window processing method\n",
        "        )\n",
        "        spec = tf.abs(stft)                   # magnitude of the full spectrogram\n",
        "        return tf.math.log1p(spec)            # log-spectrogram"
      ],
      "metadata": {
        "id": "_gp-v7g6CJU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "h9dfad2-Ehg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_fft = 1024\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Input(shape=(None,), dtype=tf.float32),  # raw waveform\n",
        "    Spectrogram(n_fft=n_fft, hop_length=512),       # log spectrogram\n",
        "\n",
        "    # add channel dimension for Conv2D\n",
        "    layers.Lambda(lambda x: tf.expand_dims(x, -1)),  # (time, freq, 1)\n",
        "\n",
        "    # CNN block\n",
        "    layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    # reshape for RNN: flatten freq*channels per timestep\n",
        "    layers.TimeDistributed(layers.Flatten()),\n",
        "\n",
        "    # RNN block (switched from GRU to SimpleRNN)\n",
        "    layers.Bidirectional(layers.SimpleRNN(64, return_sequences=True)),\n",
        "    layers.Bidirectional(layers.SimpleRNN(64)),\n",
        "\n",
        "    # Output\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy', # use BCE as the loss function as we only have 2 classes\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary() # For debugging purposes\n"
      ],
      "metadata": {
        "id": "ekr1isz8Ejld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "Ea4nALNQJ16e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure model is saved at regular checkpoints during training\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    \"model_weights.keras\",    # filename\n",
        "    save_best_only=False,     # save all checkpoints\n",
        "    save_weights_only=False,  # save model architecture as well as model weights\n",
        "    verbose=1                 # print short message each time model is saved\n",
        ")\n",
        "\n",
        "# train model\n",
        "model.fit(\n",
        "    train_dataset,                # train it on the training dataset\n",
        "    validation_data=test_dataset,  # specify validation data so that Keras can test the model on unseen data at the end of each epoch\n",
        "    epochs=EPOCHS,                # specify number of iterations throught the dataset\n",
        "    callbacks=[checkpoint_cb]     # functions to run at the end of each epoch\n",
        ")"
      ],
      "metadata": {
        "id": "GaIuiHggJ3vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model from file\n",
        "If you have an existing saved model, you can run it from the file in python like this..."
      ],
      "metadata": {
        "id": "7A96YeHmSjmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from a file\n",
        "model = load_model(\n",
        "    \"my_full_model_epoch_01.h5\",                # path to the saved model\n",
        "    custom_objects={\"Spectrogram\": Spectrogram} # include the custom spectrogram layer\n",
        ")\n",
        "\n",
        "model.evaluate(test_dataset)\n",
        "# predictions = model.predict(my_dataset) # use line to actually use your model"
      ],
      "metadata": {
        "id": "kylzGGqnStdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...or you can export it to a tensorflow lite model for use on a micrcontroller as seen below. Once you have generated your `.tflite` file, run `xxd -i model.tflite > model_data.cc` from a terminal in the same directory as your `.tflite` file to convert it to a C array. This means it's ready to run from C/C++ code which can be uploaded to a micrcontroller as firmware."
      ],
      "metadata": {
        "id": "nVTrdtoJ3ver"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(\"model_weights.keras\")\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite)"
      ],
      "metadata": {
        "id": "0PcTOlEg35Wf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}